# -*- coding: utf-8 -*-
"""distilbert_based_uncased

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CegjQz64eee6RSi4lTP-DIQ_i-o--Jtz
"""

!pip install transformers

from transformers import DistilBertTokenizer
from transformers import TFDistilBertForSequenceClassification
import tensorflow as tf
import pandas as pd
import json
from transformers import AutoTokenizer

#loads cleaned tweets as a dataframe
df = pd.read_csv('supervised_clean.csv') # Change path to your download location
df.head()

#plots number of tweets by classes
df2 = df.groupby(['labels'])['labels'].count().reset_index(name='counts')
plot = df2.plot.bar(x='labels', y='counts', rot=0)
plot.set_title('Figure 1: Number of tweets per label')

data_texts = df["text"].to_list() # Features (not-tokenized yet)
data_labels = df["labels"].to_list() # Lables

from sklearn.model_selection import train_test_split

# Split Train and Validation data
train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.30)

# Keep some data for inference (testing)
train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.10)

#tokenizer given by distilbert
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

#tokenize train dataset and validation dataset with no truncation or padding
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)

#transform train and validation datasets into tensor dataset
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))

#load model with metrics to be used
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=9)
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])

#shuffles data and fine tunes model with 4 epochs and batch size of 16
model.fit(train_dataset.shuffle(750).batch(16), epochs=4, batch_size=16,
          validation_data=val_dataset.shuffle(750).batch(16))

#saves model
save_directory = "/saved_models" # change this to your preferred location

model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

#get predicted label for the validation tweets and add to array
y_pred = []
for i in range(len(val_texts)):
  predict_input = tokenizer.encode(val_texts[i],
                                 truncation=True,
                                 padding=True,
                                 return_tensors="tf")
  
  output = model(predict_input)[0]
  prediction_value = tf.argmax(output, axis=1).numpy()[0]
  y_pred.append(prediction_value)

from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score

#prints metrics for validation dataset
print("Accuracy:",accuracy_score(val_labels, y_pred))
print("F1-score:",f1_score(val_labels, y_pred, average='weighted'))
print("Recall:",recall_score(val_labels, y_pred, average='weighted'))
print("Precision:",precision_score(val_labels, y_pred, average='weighted'))

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

#plots confusion matrix for each label
cm = confusion_matrix(val_labels, y_pred, labels=[0,1,2,3,4,5,6,7,8])

cm_display = ConfusionMatrixDisplay(cm, display_labels=[0,1,2,3,4,5,6,7,8]).plot()

