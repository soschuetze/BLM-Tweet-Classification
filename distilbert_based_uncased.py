# -*- coding: utf-8 -*-
"""distilbert_based_uncased

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CegjQz64eee6RSi4lTP-DIQ_i-o--Jtz
"""

!pip install transformers

from transformers import DistilBertTokenizer
from transformers import TFDistilBertForSequenceClassification
import tensorflow as tf
import pandas as pd
import json
import gc
from transformers import AutoTokenizer

df = pd.read_csv('supervised_clean.csv') # Change path to your download location
df.head()

df['labels'].unique()

df2 = df.groupby(['labels'])['labels'].count().reset_index(name='counts')

plot = df2.plot.bar(x='labels', y='counts', rot=0)
plot.set_title('Figure 1: Number of tweets per label')

data_texts = df["text"].to_list() # Features (not-tokenized yet)
data_labels = df["labels"].to_list() # Lables

from sklearn.model_selection import train_test_split

# Split Train and Validation data
train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.30)

# Keep some data for inference (testing)
train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.10)

import numpy as np
x = np.array(test_labels)
print(np.unique(x))

import numpy as np
x = np.array(train_labels)
print(np.unique(x))

len(test_texts)

len(val_texts)

len(train_texts)

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=9)


optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])

model.fit(train_dataset.shuffle(750).batch(16), epochs=4, batch_size=16,
          validation_data=val_dataset.shuffle(750).batch(16))

save_directory = "/saved_models" # change this to your preferred location

model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

y_pred = []
for i in range(len(val_texts)):
  predict_input = tokenizer.encode(val_texts[i],
                                 truncation=True,
                                 padding=True,
                                 return_tensors="tf")
  
  output = model(predict_input)[0]
  prediction_value = tf.argmax(output, axis=1).numpy()[0]
  y_pred.append(prediction_value)

len(y_pred)

from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score

print("Accuracy:",accuracy_score(val_labels, y_pred))
print("F1-score:",f1_score(val_labels, y_pred, average='weighted'))
print("Recall:",recall_score(val_labels, y_pred, average='weighted'))
print("Precision:",precision_score(val_labels, y_pred, average='weighted'))

from huggingface_hub import notebook_login

notebook_login()

model.push_to_hub("distilbert-blm-tweets")

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

cm = confusion_matrix(val_labels, y_pred, labels=[0,1,2,3,4,5,6,7,8])

cm_display = ConfusionMatrixDisplay(cm, display_labels=[0,1,2,3,4,5,6,7,8]).plot()

